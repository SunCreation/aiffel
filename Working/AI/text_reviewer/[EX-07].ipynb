{"cells":[{"cell_type":"code","execution_count":1,"source":["import pandas as pd\n","import urllib.request\n","# %matplotlib inline\n","import matplotlib.pyplot as plt\n","import re\n","from konlpy.tag import Okt # 언젠가 보자...\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from collections import Counter\n","\n","# 데이터를 읽어봅시다.\n","train_path = 'data/nsmc/ratings_train.txt'\n","test_path = 'data/nsmc/ratings_test.txt'\n","train_data = pd.read_table(train_path)\n","test_data = pd.read_table(test_path)\n","\n","train_data.head()"],"outputs":[{"output_type":"stream","name":"stderr","text":["2021-10-28 22:37:05.512902: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2021-10-28 22:37:05.512987: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9976970</td>\n","      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3819312</td>\n","      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10265843</td>\n","      <td>너무재밓었다그래서보는것을추천한다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9045019</td>\n","      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6483659</td>\n","      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                           document  label\n","0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n","1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n","3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n","4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"]},"metadata":{},"execution_count":1}],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["# 데이터 전처리 입니다.\n","from konlpy.tag import Mecab\n","tokenizer = Mecab()\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","num_words=20000\n","def load_data(train_data, test_data, num_words=num_words):\n","    # [[YOUR CODE]]\n","    train_data.drop_duplicates(subset=['document'], inplace=True)\n","    train_data = train_data.dropna(how = 'any') \n","    test_data.drop_duplicates(subset=['document'], inplace=True)\n","    test_data = test_data.dropna(how = 'any') \n","    \n","    X_train = []\n","    for sentence in train_data['document']:\n","        temp_X = tokenizer.morphs(sentence) # 토큰화\n","        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","        X_train.append(temp_X)\n","\n","    X_test = []\n","    for sentence in test_data['document']:\n","        temp_X = tokenizer.morphs(sentence) # 토큰화\n","        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","        X_test.append(temp_X)\n","    \n","    words = np.concatenate(X_train).tolist()\n","    counter = Counter(words)\n","    counter = counter.most_common(10000-4)\n","    vocab = ['', '', '', ''] + [key for key, _ in counter]\n","    word_to_index = {word:index for index, word in enumerate(vocab)}\n","        \n","    def wordlist_to_indexlist(wordlist):\n","        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n","        \n","    X_train = list(map(wordlist_to_indexlist, X_train))\n","    X_test = list(map(wordlist_to_indexlist, X_test))\n","        \n","    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n","    \n","X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["index_to_word = {index:word for word, index in word_to_index.items()}"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n","# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n","def get_encoded_sentence(sentence, word_to_index):\n","    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n","\n","# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n","def get_encoded_sentences(sentences, word_to_index):\n","    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n","\n","# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n","def get_decoded_sentence(encoded_sentence, index_to_word):\n","    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n","\n","# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n","def get_decoded_sentences(encoded_sentences, index_to_word):\n","    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["import gensim\n","word2vec_path = 'data/ko.bin'\n","word2vec = gensim.models.Word2Vec.load(word2vec_path)\n","vector = word2vec['가방']\n","vector     # 무려 300dim의 워드 벡터입니다."],"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipykernel_24817/3916257801.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  vector = word2vec['가방']\n"]},{"output_type":"execute_result","data":{"text/plain":["array([ 0.44337696,  0.33067897,  0.00423185,  0.09941821,  0.24922162,\n","       -1.5483421 ,  0.7179065 ,  0.44619754,  0.26341715, -0.7557755 ,\n","        1.6730225 ,  1.5506932 ,  0.41791508,  0.23653346,  0.3370173 ,\n","       -0.5054976 ,  0.5848394 ,  0.86017835, -0.8320465 , -1.1213914 ,\n","       -0.6356562 ,  0.00758559,  0.7700866 , -0.15168501, -0.49680716,\n","       -0.459246  ,  1.3269166 , -0.02057629, -0.01498494,  0.36286223,\n","       -0.38423136, -0.48272285,  0.02059898, -0.4342361 ,  0.0808625 ,\n","        0.9751739 ,  0.39841947, -0.00312074, -0.01050324,  0.06396951,\n","       -0.858028  , -0.7100742 ,  0.40870246, -0.90541303,  0.0487923 ,\n","       -0.5760212 ,  0.32594004, -0.31112763, -0.46094155, -1.073029  ,\n","        0.7887254 ,  0.36984107,  0.37219578, -0.6615945 , -1.2793318 ,\n","        0.31662583, -0.19714019,  0.08608919,  0.46863657, -0.26394662,\n","        0.28440842, -0.30910465, -0.63530475, -0.3965939 , -0.01063732,\n","        0.05109216, -0.48139465, -0.08882087,  0.52212125,  0.15788199,\n","        0.07106787, -1.0733455 , -1.145272  , -0.48883146, -0.00367811,\n","       -0.97645926, -0.02421511, -0.37611637, -0.19997485, -0.8955466 ,\n","        0.3630227 , -0.9486312 , -0.42946544, -1.3694638 , -1.7440723 ,\n","        0.15074047,  0.6949235 ,  0.60510355, -0.38132933,  0.9760045 ,\n","       -0.39751506, -0.51470864, -0.23493198,  0.30371583,  0.07490665,\n","        0.39290577,  0.72625834,  0.20400128,  0.07260764, -0.4941408 ,\n","        0.67861706, -0.11482497,  0.9862703 , -0.1410723 ,  0.15053551,\n","        0.26696518, -0.01870198, -0.4393796 ,  0.5979941 , -0.5019096 ,\n","        0.7662593 ,  0.4650255 , -0.61541086, -0.24314326, -0.3504104 ,\n","        0.14421076, -1.3872604 ,  0.29320914, -0.5222191 , -0.42146707,\n","       -0.00383861,  0.2552312 , -0.11143502, -0.8957899 , -0.2531076 ,\n","        0.20485379, -0.42782512,  0.48323622,  0.18139155,  0.05814376,\n","        0.10523599,  0.18480928, -0.18388283,  0.6878498 ,  0.54367787,\n","        0.14691402,  0.6224243 , -1.3333883 ,  0.16249013, -0.7052882 ,\n","        0.00892893, -0.09547003,  0.01875778, -0.08980865, -0.4885307 ,\n","        0.24844994, -0.7590728 , -1.4442575 , -0.08777061, -0.43568268,\n","       -0.04396483,  0.51605606,  0.2919627 ,  0.17845358,  0.6854313 ,\n","       -0.74137753, -0.3417879 ,  0.653703  , -0.503081  , -1.4502912 ,\n","       -0.5963747 , -0.6250137 ,  0.06528363, -0.74652314,  0.2630199 ,\n","       -0.18226545,  0.72657067, -0.7446869 ,  0.3288398 ,  0.01224568,\n","       -0.98508954, -0.03788053, -0.01006415, -0.0678578 ,  0.3774199 ,\n","       -0.2634148 , -1.739302  , -0.04227779, -0.06153663,  0.10676163,\n","        0.02270738, -0.41364536,  0.11944565,  0.55895895, -0.83718914,\n","        1.1367677 , -0.90956205,  0.13838798, -0.20970435,  0.23744166,\n","       -0.66164726, -0.06205282,  1.7939919 ,  0.7531474 , -0.39389208,\n","       -0.23972218,  0.2936302 , -1.20727   , -0.5895726 , -0.8665029 ],\n","      dtype=float32)"]},"metadata":{},"execution_count":5}],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["word2vec.similar_by_word(\"피자\")"],"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipykernel_24817/1858146928.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n","  word2vec.similar_by_word(\"피자\")\n"]},{"output_type":"execute_result","data":{"text/plain":["[('소시지', 0.7531833052635193),\n"," ('햄버거', 0.7404618263244629),\n"," ('샐러드', 0.7318634986877441),\n"," ('파스타', 0.7314548492431641),\n"," ('팬케이크', 0.7270224690437317),\n"," ('치즈', 0.7189904451370239),\n"," ('요구르트', 0.7089414596557617),\n"," ('샌드위치', 0.7071967720985413),\n"," ('토스트', 0.6964404582977295),\n"," ('치킨', 0.6932948231697083)]"]},"metadata":{},"execution_count":6}],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["from silence_tensorflow import silence_tensorflow\n","silence_tensorflow()\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["len(vector)     # 무려 300dim의 워드 벡터입니다."],"outputs":[{"output_type":"execute_result","data":{"text/plain":["200"]},"metadata":{},"execution_count":8}],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n","word_vector_dim = 200  # 워드 벡터의 차원수\n","embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n","\n","# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n","for i in range(4,vocab_size):\n","    if index_to_word[i] in word2vec:\n","        embedding_matrix[i] = word2vec[index_to_word[i]]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["total_data_text = list(X_train) + list(X_test)\n","# 텍스트데이터 문장길이의 리스트를 생성한 후\n","num_tokens = [len(tokens) for tokens in total_data_text]\n","num_tokens = np.array(num_tokens)\n","# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n","print('문장길이 평균 : ', np.mean(num_tokens))\n","print('문장길이 최대 : ', np.max(num_tokens))\n","print('문장길이 표준편차 : ', np.std(num_tokens))\n","\n","# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n","max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n","maxlen = int(max_tokens)\n","print('pad_sequences maxlen : ', maxlen)\n","print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"],"outputs":[{"output_type":"stream","name":"stdout","text":["문장길이 평균 :  15.969376315021577\n","문장길이 최대 :  116\n","문장길이 표준편차 :  12.843535456326455\n","pad_sequences maxlen :  41\n","전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"]}],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["from tensorflow.keras.initializers import Constant\n","\n","# vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n","# word_vector_dim = 200  # 워드 벡터의 차원 수 \n","\n","# 모델 구성\n","model = keras.Sequential()\n","model.add(keras.layers.Embedding(vocab_size, \n","                                 word_vector_dim, \n","                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n","                                 input_length=maxlen, \n","                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n","model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n","model.add(keras.layers.MaxPooling1D(5))\n","model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n","model.add(keras.layers.GlobalMaxPooling1D())\n","model.add(keras.layers.Dense(8, activation='relu'))\n","model.add(keras.layers.Dense(1, activation='sigmoid')) \n","\n","model.summary()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 41, 200)           2000000   \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, 35, 16)            22416     \n","_________________________________________________________________\n","max_pooling1d (MaxPooling1D) (None, 7, 16)             0         \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 1, 16)             1808      \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 16)                0         \n","_________________________________________________________________\n","dense (Dense)                (None, 8)                 136       \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 9         \n","=================================================================\n","Total params: 2,024,369\n","Trainable params: 2,024,369\n","Non-trainable params: 0\n","_________________________________________________________________\n"]},{"output_type":"stream","name":"stderr","text":["2021-10-28 22:53:19.553665: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2021-10-28 22:53:19.555536: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n","2021-10-28 22:53:19.556118: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Gift-of-Mia): /proc/driver/nvidia/version does not exist\n","2021-10-28 22:53:19.563802: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["len(X_train)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["146182"]},"metadata":{},"execution_count":15}],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["# validation set 10000건 분리\n","X_val = X_train[:10000]   \n","y_val = y_train[:10000]\n","\n","# validation set을 제외한 나머지 15000건\n","partial_X_train = X_train[10000:]  \n","partial_y_train = y_train[10000:]\n","\n","print(len(partial_X_train))\n","print(len(partial_y_train))\n","X_train[0]"],"outputs":[{"output_type":"stream","name":"stdout","text":["136182\n","136182\n"]},{"output_type":"execute_result","data":{"text/plain":["[32, 74, 919, 4, 4, 39, 228, 20, 33, 748]"]},"metadata":{},"execution_count":18}],"metadata":{}},{"cell_type":"code","execution_count":22,"source":["# 데이터 전처리 입니다.\n","from konlpy.tag import Mecab\n","tokenizer = Mecab()\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","num_words=20000\n","def load_data(train_data, test_data, num_words=num_words):\n","    # [[YOUR CODE]]\n","    train_data.drop_duplicates(subset=['document'], inplace=True)\n","    train_data = train_data.dropna(how = 'any') \n","    test_data.drop_duplicates(subset=['document'], inplace=True)\n","    test_data = test_data.dropna(how = 'any') \n","    \n","    X_train = []\n","    for sentence in train_data['document']:\n","        temp_X = tokenizer.morphs(sentence) # 토큰화\n","        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","        X_train.append(temp_X)\n","\n","    X_test = []\n","    for sentence in test_data['document']:\n","        temp_X = tokenizer.morphs(sentence) # 토큰화\n","        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","        X_test.append(temp_X)\n","    \n","    words = np.concatenate(X_train).tolist()\n","    counter = Counter(words)\n","    counter = counter.most_common(10000-4)\n","    vocab = ['', '', '', ''] + [key for key, _ in counter]\n","    word_to_index = {word:index for index, word in enumerate(vocab)}\n","        \n","    def wordlist_to_indexlist(wordlist):\n","        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n","        \n","    X_train = list(map(wordlist_to_indexlist, X_train))\n","    X_test = list(map(wordlist_to_indexlist, X_test))\n","        \n","    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n","    \n","X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":25,"source":["# 데이터 전처리 입니다.\n","from konlpy.tag import Mecab\n","tokenizer = Mecab()\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","num_words=20000\n","def load_data(train_data, test_data, num_words=num_words):\n","    # [[YOUR CODE]]\n","    train_data.drop_duplicates(subset=['document'], inplace=True)\n","    train_data = train_data.dropna(how = 'any') \n","    test_data.drop_duplicates(subset=['document'], inplace=True)\n","    test_data = test_data.dropna(how = 'any') \n","    \n","    X_train = []\n","    for sentence in train_data['document']:\n","        temp_X = tokenizer.morphs(sentence) # 토큰화\n","        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","        X_train.append(temp_X)\n","\n","    X_test = []\n","    for sentence in test_data['document']:\n","        temp_X = tokenizer.morphs(sentence) # 토큰화\n","        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","        X_test.append(temp_X)\n","    \n","    words = np.concatenate(X_train).tolist()\n","    counter = Counter(words)\n","    counter = counter.most_common(10000-4)\n","    vocab = ['', '', '', ''] + [key for key, _ in counter]\n","    word_to_index = {word:index for index, word in enumerate(vocab)}\n","        \n","    def wordlist_to_indexlist(wordlist):\n","        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n","        \n","    X_train = list(map(wordlist_to_indexlist, X_train))\n","    X_test = list(map(wordlist_to_indexlist, X_test))\n","        \n","    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n","    \n","X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":26,"source":["print(index_to_word[1])"],"outputs":[{"output_type":"stream","name":"stdout","text":["<BOS>\n"]}],"metadata":{}},{"cell_type":"code","execution_count":29,"source":["# 데이터 전처리 입니다.\n","from konlpy.tag import Mecab\n","tokenizer = Mecab()\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","num_words=20000\n","def load_data(train_data, test_data, num_words=num_words):\n","    # [[YOUR CODE]]\n","    train_data.drop_duplicates(subset=['document'], inplace=True)\n","    train_data = train_data.dropna(how = 'any') \n","    test_data.drop_duplicates(subset=['document'], inplace=True)\n","    test_data = test_data.dropna(how = 'any') \n","    \n","    X_train = []\n","    for sentence in train_data['document']:\n","        temp_X = tokenizer.morphs(sentence) # 토큰화\n","        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","        X_train.append(temp_X)\n","\n","    X_test = []\n","    for sentence in test_data['document']:\n","        temp_X = tokenizer.morphs(sentence) # 토큰화\n","        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n","        X_test.append(temp_X)\n","    \n","    words = np.concatenate(X_train).tolist()\n","    counter = Counter(words)\n","    counter = counter.most_common(10000-4)\n","    vocab = ['', '', '', ''] + [key for key, _ in counter]\n","    word_to_index = {word:index for index, word in enumerate(vocab)}\n","        \n","    def wordlist_to_indexlist(wordlist):\n","        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n","        \n","    X_train = list(map(wordlist_to_indexlist, X_train))\n","    X_test = list(map(wordlist_to_indexlist, X_test))\n","        \n","    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n","    \n","X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":30,"source":["print(word_to_index['하']) \n","# print(index_to_word[1])"],"outputs":[{"output_type":"stream","name":"stdout","text":["8\n"]}],"metadata":{}},{"cell_type":"code","execution_count":31,"source":["print(word_to_index['하']) \n","# print(index_to_word[1])\n","index_to_word = {index:word for word, index in word_to_index.items()}"],"outputs":[{"output_type":"stream","name":"stdout","text":["8\n"]}],"metadata":{}},{"cell_type":"code","execution_count":34,"source":["# print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n","# print(word_to_index[2])  # 4 이 출력됩니다. \n","print(index_to_word[4])     # 'the' 가 출력됩니다."],"outputs":[{"output_type":"stream","name":"stdout","text":[".\n"]}],"metadata":{}},{"cell_type":"code","execution_count":35,"source":["# print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n","# print(word_to_index[2])  # 4 이 출력됩니다. \n","print(index_to_word[3])     # 'the' 가 출력됩니다."],"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":36,"source":["# print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n","# print(word_to_index[2])  # 4 이 출력됩니다. \n","print(index_to_word[3])     # 'the' 가 출력됩니다."],"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}],"metadata":{}},{"cell_type":"code","execution_count":38,"source":["# word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n","\n","# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n","word_to_index[\"<PAD>\"] = 0\n","word_to_index[\"<BOS>\"] = 1\n","word_to_index[\"<UNK>\"] = 2  # unknown\n","word_to_index[\"<UNUSED>\"] = 3\n","\n","index_to_word = {index:word for word, index in word_to_index.items()}\n","\n","print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n","print(word_to_index['하'])  # 4 이 출력됩니다. \n","print(index_to_word[4])     # 'the' 가 출력됩니다."],"outputs":[{"output_type":"stream","name":"stdout","text":["<BOS>\n","8\n",".\n"]}],"metadata":{}},{"cell_type":"code","execution_count":40,"source":["print(X_train.shape,X_test.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["(146182, 41) (49157, 41)\n"]}],"metadata":{}},{"cell_type":"code","execution_count":41,"source":["# validation set 10000건 분리\n","X_val = X_train[:10000]   \n","y_val = y_train[:10000]\n","\n","# validation set을 제외한 나머지 15000건\n","partial_X_train = X_train[10000:]  \n","partial_y_train = y_train[10000:]\n","\n","print(partial_X_train.shape)\n","print(partial_y_train.shape)\n","X_train[0]"],"outputs":[{"output_type":"stream","name":"stdout","text":["(136182, 41)\n","(136182,)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,  32,  74, 919,   4,   4,  39, 228,  20,\n","        33, 748], dtype=int32)"]},"metadata":{},"execution_count":41}],"metadata":{}},{"cell_type":"code","execution_count":44,"source":["model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model.fit(partial_X_train,\n","                    partial_y_train,\n","                    epochs=3,\n","                    batch_size=512,\n","                    validation_data=(X_val,y_val),\n","                    verbose=1)"],"outputs":[{"output_type":"stream","name":"stderr","text":["2021-10-28 23:12:48.172687: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","266/266 [==============================] - 45s 167ms/step - loss: 0.5471 - accuracy: 0.7056 - val_loss: 0.4272 - val_accuracy: 0.8070\n","Epoch 2/3\n","266/266 [==============================] - 57s 213ms/step - loss: 0.3845 - accuracy: 0.8294 - val_loss: 0.3647 - val_accuracy: 0.8435\n","Epoch 3/3\n","266/266 [==============================] - 48s 180ms/step - loss: 0.3281 - accuracy: 0.8604 - val_loss: 0.3476 - val_accuracy: 0.8505\n"]}],"metadata":{}},{"cell_type":"code","execution_count":46,"source":["results = model.evaluate(X_test,  y_test, verbose=2)\n","\n","print(results)"],"outputs":[{"output_type":"stream","name":"stdout","text":["1537/1537 - 5s - loss: 0.3612 - accuracy: 0.8441\n","[0.3612341582775116, 0.8440913558006287]\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}