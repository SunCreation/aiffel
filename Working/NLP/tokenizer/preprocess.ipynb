{"cells":[{"cell_type":"code","execution_count":1,"source":["def pad_punctuation(sentence, punc):\n","    for p in punc:\n","        sentence = sentence.replace(p, \" \" + p + \" \")\n","\n","    return sentence\n","\n","sentence = \"Hi, my name is john.\"\n","\n","print(pad_punctuation(sentence, [\".\", \"?\", \"!\", \",\"]))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Hi ,  my name is john . \n"]}],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["sentence = \"First, open the first chapter.\"\n","\n","print(sentence.lower())"],"outputs":[{"output_type":"stream","name":"stdout","text":["first, open the first chapter.\n"]}],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["sentence = \"First, open the first chapter.\"\n","\n","print(sentence.upper())"],"outputs":[{"output_type":"stream","name":"stdout","text":["FIRST, OPEN THE FIRST CHAPTER.\n"]}],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["import re\n","\n","sentence = \"He is a ten-year-old boy.\"\n","sentence = re.sub(\"([^a-zA-Z.,?!])\", \" \", sentence)\n","\n","print(sentence)"],"outputs":[{"output_type":"stream","name":"stdout","text":["He is a ten year old boy.\n"]}],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["# From The Project Gutenberg\n","# (https://www.gutenberg.org/files/2397/2397-h/2397-h.htm)\n","\n","corpus = \\\n","\"\"\"\n","In the days that followed I learned to spell in this uncomprehending way a great many words, among them pin, hat, cup and a few verbs like sit, stand and walk. \n","But my teacher had been with me several weeks before I understood that everything has a name.\n","One day, we walked down the path to the well-house, attracted by the fragrance of the honeysuckle with which it was covered. \n","Some one was drawing water and my teacher placed my hand under the spout. \n","As the cool stream gushed over one hand she spelled into the other the word water, first slowly, then rapidly. \n","I stood still, my whole attention fixed upon the motions of her fingers. \n","Suddenly I felt a misty consciousness as of something forgotten—a thrill of returning thought; and somehow the mystery of language was revealed to me. \n","I knew then that \"w-a-t-e-r\" meant the wonderful cool something that was flowing over my hand. \n","That living word awakened my soul, gave it light, hope, joy, set it free! \n","There were barriers still, it is true, but barriers that could in time be swept away.\n","\"\"\" \n","\n","def cleaning_text(text, punc, regex):\n","    # 노이즈 유형 (1) 문장부호 공백추가\n","    text = re.sub(punc, r\" \\1 \", text)\n","\n","    # 노이즈 유형 (2), (3) 소문자화 및 특수문자 제거\n","    text = re.sub(regex, \" \", text).lower()\n","\n","    return text\n","\n","print(cleaning_text(corpus, r\"([.,!?])\", r\"([^a-zA-Z0-9.,?!\\n])\"))"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n","in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n","but my teacher had been with me several weeks before i understood that everything has a name . \n","one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n","some one was drawing water and my teacher placed my hand under the spout .  \n","as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n","i stood still ,  my whole attention fixed upon the motions of her fingers .  \n","suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n","i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n","that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n","there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n","\n"]}],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["corpus = \\\n","\"\"\"\n","in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n","but my teacher had been with me several weeks before i understood that everything has a name . \n","one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n","some one was drawing water and my teacher placed my hand under the spout .  \n","as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n","i stood still ,  my whole attention fixed upon the motions of her fingers .  \n","suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n","i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n","that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n","there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n","\"\"\"\n","\n","tokens = corpus.split()\n","\n","print(\"문장이 포함하는 Tokens:\", tokens)"],"outputs":[{"output_type":"stream","name":"stdout","text":["문장이 포함하는 Tokens: ['in', 'the', 'days', 'that', 'followed', 'i', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words', ',', 'among', 'them', 'pin', ',', 'hat', ',', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit', ',', 'stand', 'and', 'walk', '.', 'but', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'i', 'understood', 'that', 'everything', 'has', 'a', 'name', '.', 'one', 'day', ',', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well', 'house', ',', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered', '.', 'some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout', '.', 'as', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water', ',', 'first', 'slowly', ',', 'then', 'rapidly', '.', 'i', 'stood', 'still', ',', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers', '.', 'suddenly', 'i', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten', 'a', 'thrill', 'of', 'returning', 'thought', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me', '.', 'i', 'knew', 'then', 'that', 'w', 'a', 't', 'e', 'r', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand', '.', 'that', 'living', 'word', 'awakened', 'my', 'soul', ',', 'gave', 'it', 'light', ',', 'hope', ',', 'joy', ',', 'set', 'it', 'free', '!', 'there', 'were', 'barriers', 'still', ',', 'it', 'is', 'true', ',', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away', '.']\n"]}],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["import khaiii\n","\n","api = khaiii.KhaiiiApi()\n","api.open()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["# Khaiii를 konlpy tokenizer처럼 사용하기 위한 wrapper class입니다. \n","\n","class Khaiii():\n","    def pos(self, phrase, flatten=True, join=False):\n","        \"\"\"POS tagger.\n","\n","        :param flatten: If False, preserves eojeols.\n","        :param join: If True, returns joined sets of morph and tag.\n","\n","        \"\"\"\n","        sentences = phrase.split('\\n')\n","        morphemes = []\n","        if not sentences:\n","            return morphemes\n","\n","        for sentence in sentences:\n","            for word in api.analyze(sentence):\n","                result = [(m.lex, m.tag) for m in word.morphs]\n","                if join:\n","                    result = ['{}/{}'.format(m.lex, m.tag) for m in word.morphs]\n","\n","                morphemes.append(result)\n","\n","        if flatten:\n","            return sum(morphemes, [])\n","\n","        return morphemes"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["tokenizer_list = [Hannanum(),Kkma(),Komoran(),Mecab(),Okt(),Khaiii()]\n","\n","kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\n","\n","for tokenizer in tokenizer_list:\n","    print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))"],"outputs":[{"output_type":"error","ename":"Exception","evalue":"Install MeCab in order to use it: http://konlpy.org/en/latest/install/","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m~/Working/lib/python3.8/site-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-d %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/data/tagset/mecab.json'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstallpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Tagger' is not defined","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_30204/198139351.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHannanum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mKkma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mKomoran\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMecab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mOkt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mKhaiii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkor_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Working/lib/python3.8/site-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The MeCab dictionary does not exist at \"%s\". Is the dictionary correctly installed?\\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab(\\'/some/dic/path\\')\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Install MeCab in order to use it: http://konlpy.org/en/latest/install/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"]}],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["import re, collections\n","\n","# 임의의 데이터에 포함된 단어들입니다.\n","# 우측의 정수는 임의의 데이터에 해당 단어가 포함된 빈도수입니다.\n","vocab = {\n","    'l o w '      : 5,\n","    'l o w e r '  : 2,\n","    'n e w e s t ': 6,\n","    'w i d e s t ': 3\n","}\n","\n","num_merges = 5\n","\n","def get_stats(vocab):\n","    \"\"\"\n","    단어 사전을 불러와\n","    단어는 공백 단위로 쪼개어 문자 list를 만들고\n","    빈도수와 쌍을 이루게 합니다. (symbols)\n","    \"\"\"\n","    pairs = collections.defaultdict(int)\n","    \n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","\n","        for i in range(len(symbols) - 1):             # 모든 symbols를 확인하여 \n","            pairs[symbols[i], symbols[i + 1]] += freq  # 문자 쌍의 빈도수를 저장합니다. \n","        \n","    return pairs\n","\n","def merge_vocab(pair, v_in):\n","    v_out = {}\n","    bigram = re.escape(' '.join(pair))\n","    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n","    \n","    for word in v_in:\n","        w_out = p.sub(''.join(pair), word)\n","        v_out[w_out] = v_in[word]\n","        \n","    return v_out, pair[0] + pair[1]\n","\n","token_vocab = []\n","\n","for i in range(num_merges):\n","    print(\">> Step {0}\".format(i + 1))\n","    \n","    pairs = get_stats(vocab)\n","    best = max(pairs, key=pairs.get)  # 가장 많은 빈도수를 가진 문자 쌍을 반환합니다.\n","    vocab, merge_tok = merge_vocab(best, vocab)\n","    print(\"다음 문자 쌍을 치환:\", merge_tok)\n","    print(\"변환된 Vocab:\\n\", vocab, \"\\n\")\n","    \n","    token_vocab.append(merge_tok)\n","    \n","print(\"Merge Vocab:\", token_vocab)"],"outputs":[{"output_type":"stream","name":"stdout","text":[">> Step 1\n","다음 문자 쌍을 치환: es\n","변환된 Vocab:\n"," {'l o w ': 5, 'l o w e r ': 2, 'n e w es t ': 6, 'w i d es t ': 3} \n","\n",">> Step 2\n","다음 문자 쌍을 치환: est\n","변환된 Vocab:\n"," {'l o w ': 5, 'l o w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n","\n",">> Step 3\n","다음 문자 쌍을 치환: lo\n","변환된 Vocab:\n"," {'lo w ': 5, 'lo w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n","\n",">> Step 4\n","다음 문자 쌍을 치환: low\n","변환된 Vocab:\n"," {'low ': 5, 'low e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n","\n",">> Step 5\n","다음 문자 쌍을 치환: ne\n","변환된 Vocab:\n"," {'low ': 5, 'low e r ': 2, 'ne w est ': 6, 'w i d est ': 3} \n","\n","Merge Vocab: ['es', 'est', 'lo', 'low', 'ne']\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"kernelspec":{"name":"python3","display_name":"Python 3.8.10 64-bit ('Working': venv)"},"interpreter":{"hash":"026aceb1435085fcef523649cdfc9385a4a55dbc5c65435142607853821fa50a"}}}