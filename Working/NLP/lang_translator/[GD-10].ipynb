{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "my_translator\n",
    "=\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# 목차\n",
    "## 1. 목표 및 의의\n",
    "## 2. 이론\n",
    "## 3. 코드\n",
    "## 4. 회고\n",
    "\n",
    "<br>\n",
    "\n",
    "# 1. 목표 및 의의\n",
    "## 1) Transformer를 낱낱히 뜯어본다.\n",
    "## 2) 스스로 사용할만한 번역기에 도전해본다...!!\n",
    "\n",
    "<br>\n",
    "\n",
    "# 2. 이론\n",
    "## Transformer\n",
    "\n",
    "<br>\n",
    "\n",
    "# 3. 코드"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mon Jan 10 08:11:13 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   22C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn # Attention 시각화\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def positional_encoding(pos_len, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos_len)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:,0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:,1::2])\n",
    "    return sinusoid_table\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scale_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        # scaled qk 구하기\n",
    "\n",
    "        QK_T = tf.matmul(Q, K, transpose_b=True)\n",
    "        scaled_qk = QK_T / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)\n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, *xs): # num_heads는 self에 있으니까 받을 필요 없음\n",
    "        # MultiHead에 넣을려고 분할\n",
    "        # x: [ batch x length x embedding_dimension ]\n",
    "        # return: [ batch x heads x length x embedding_dimension]\n",
    "        split_xs = []\n",
    "        for x in xs:\n",
    "            a,b,c = x.shape\n",
    "\n",
    "            split_x = tf.reshape(x,(a,b,self.num_heads,self.depth))\n",
    "            split_x = tf.transpose(split_x, (0,2,1,3))            \n",
    "            split_xs.append(split_x)\n",
    "            \n",
    "        return split_xs\n",
    "\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # 분할 계산을 마치고, 임베딩을 다시 결합한다.\n",
    "        # x: [ batch x heads x length x depth ]\n",
    "        # return: [ batch x length x emb ]\n",
    "        x = tf.transpose(x,(0,2,1,3))\n",
    "        a,b,c,d = x.shape\n",
    "\n",
    "        concat_x = tf.reshape(x, (a,b,c*d))\n",
    "\n",
    "        return concat_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask=None): #[batch x len x 512 ]\n",
    "        # 1: Linear_in(Q,K,V) -> WQ, WK, WV\n",
    "        wq = self.W_q(Q)\n",
    "        wk = self.W_k(K)\n",
    "        wv = self.W_v(V)\n",
    "\n",
    "        # 2: split heads\n",
    "        W_qkv_split = self.split_heads(wq,wk,wv)\n",
    "\n",
    "        # 3: scaled dot product attention\n",
    "        out, attention_weights = self.scale_dot_product_attention(*W_qkv_split, mask)\n",
    "\n",
    "        # 4: Combine Heads(out) -> out\n",
    "        out = self.combine_heads(out)\n",
    "\n",
    "        # 5: Linear_out(out) -> out\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Position_wise_FFN(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(Position_wise_FFN, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self,x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "        return out\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = Position_wise_FFN(d_model,d_ff)\n",
    "\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask, training=False):\n",
    "        # Multi-Head Attention\n",
    "        residual = x\n",
    "\n",
    "        out = self.norm1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out,out,out, mask)\n",
    "        out = self.dropout(out, training=training)\n",
    "\n",
    "        out += residual\n",
    "\n",
    "        # position wise FFN\n",
    "        residual2 = out\n",
    "        out = self.norm2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual2\n",
    "\n",
    "        return out, enc_attn\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, dff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.dec_attn = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "        self.ffn = Position_wise_FFN(d_model,dff)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x,y, causality_mask, padding_mask, training=False):\n",
    "        residual = x\n",
    "        out = self.norm1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out,out,out, padding_mask)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm2(out)\n",
    "        out, dec_enc_attn = self.dec_attn(out,y,y, causality_mask)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 dff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model,n_heads,dff,dropout) for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, x, mask, training=False):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask, training)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, dff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model,n_heads, dff, dropout) for i in range(n_layers)]\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask, training=False):\n",
    "        out = x\n",
    "        \n",
    "        dec_attns = []\n",
    "        dec_enc_attns = []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out,enc_out, causality_mask, padding_mask, training)\n",
    "            \n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers,d_model,\n",
    "                 n_heads,dff,src_vocab_size,\n",
    "                 tgt_vocab_size, pos_len,\n",
    "                 dropout=0.2, shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_embedding = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_embedding = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.positional = positional_encoding(pos_len,d_model)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, dff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, dff, dropout)\n",
    "        \n",
    "        self.out_linear = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.out_linear.set_weights(tf.transpose(self.dec_embedding.weights))\n",
    "\n",
    "    \n",
    "    def embedding(self, emb, x):\n",
    "        # share?\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.positional[np.newaxis, ...][:, :seq_len, :]\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # 1 embedding\n",
    "        enc = self.embedding(self.enc_embedding, enc_in)\n",
    "        dec = self.embedding(self.dec_embedding, dec_in)\n",
    "\n",
    "        # 2 encoder, decoder\n",
    "        enc_out, enc_attns = self.encoder(enc, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.out_linear(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "width": 426,
       "height": 208
      },
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "now_path = \"aiffel/Data/kor_en/\"\n",
    "path_to_enfile = \"data/kor_en/korean-english-park.train.en\"\n",
    "path_to_kofile = \"data/kor_en/korean-english-park.train.ko\"\n",
    "kor_path = now_path + path_to_enfile\n",
    "eng_path = now_path + path_to_kofile"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\n",
    "# 데이터 정제\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.readlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.readlines()\n",
    "    assert len(kor) == len(eng)\n",
    "    print(len(kor))\n",
    "    \n",
    "    dataset = set()\n",
    "    for i,j in zip(kor, eng):\n",
    "        i = preprocess_sentence(i)\n",
    "        j = preprocess_sentence(j)\n",
    "        dataset.add((i,j))\n",
    "    print(len(dataset))\n",
    "    cleaned_corpus = list(dataset)\n",
    "    return cleaned_corpus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import re\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'([?!,.\"])', r' \\1 ',sentence)\n",
    "    sentence = re.sub(r'[^A-zㄱ-ㅎㅏ-ㅣ가-힣0-9?!,.\"]', ' ', sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', ' ',sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "94123\n",
      "78931\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"ko\",\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "    path = 'aiffel/Data/Model/transformer/'\n",
    "    temp_file = f'{path}corpus_{lang}.temp'\n",
    "    \n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    \n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={temp_file} --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} \\\n",
    "        --unk_id={unk_id} --model_prefix={path}spm_{lang} --vocab_size={vocab_size} --model_type=bpe'\n",
    "    )\n",
    "    \n",
    "    s = spm.SentencePieceProcessor()\n",
    "    s.Load(f'{path}spm_{lang}.model')\n",
    "    print(f\"{lang}-dict_num: {20000}\")\n",
    "\n",
    "    return s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "eng, kor = zip(*cleaned_corpus)\n",
    "print(kor[0])\n",
    "ko = generate_tokenizer(kor,20000)\n",
    "en = generate_tokenizer(eng,20000, lang=\"en\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "china adopts more humane method for the death penalty 중국 , 인도적인 사형 방법 도입 200806 . 01\n",
      "ko-dict_num: 20000\n",
      "en-dict_num: 20000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def tokenize(corpus, tensorlen,voca_size):  # corpus: Tokenized Sentence's List\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=voca_size,filters='',)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=tensorlen)\n",
    "\n",
    "    return tensor, tokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\n",
    "path = 'aiffel/Data/Model/transformer/'\n",
    "temp_file = f'{path}corpus_ko.temp'\n",
    "ko_seq = []\n",
    "with open(temp_file,'r') as f:\n",
    "    for i in f.readlines():\n",
    "        ko_seq.append(ko.SampleEncodeAsPieces(i,1, 0.0))\n",
    "ko_tensor, ko_tokenizer = tokenize(ko_seq,50,20000)\n",
    "print(ko_seq[0])\n",
    "en_seq = []\n",
    "temp_file = f'{path}corpus_en.temp'\n",
    "with open(temp_file,'r') as f:\n",
    "    for i in f.readlines():\n",
    "        en_seq.append(en.SampleEncodeAsPieces(i,1, 0.0))\n",
    "en_tensor, en_tokenizer = tokenize(en_seq,50,20000)\n",
    "print(en_seq[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['▁china', '▁ad', 'op', 'ts', '▁more', '▁human', 'e', '▁me', 'th', 'od', '▁for', '▁the', '▁death', '▁pen', 'al', 'ty', '▁중국', '▁,', '▁인도', '적인', '▁사형', '▁방법', '▁도입', '▁200806', '▁.', '▁01']\n",
      "['▁provisional', '▁results', '▁had', '▁given', '▁victory', '▁to', '▁the', '▁people', '▁power', '▁party', '▁ppp', '▁,', '▁with', '▁23', '3', '▁of', '▁the', '▁480', '▁available', '▁seats', '▁.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "print(f\"ko dict voca: {len(ko_tokenizer.index_word)}\")\n",
    "print(f\"en dict voca: {len(en_tokenizer.index_word)}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ko dict voca: 20288\n",
      "en dict voca: 19066\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "print(len(ko_seq), len(en_seq))\n",
    "print(ko_seq[1], en_seq[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "78931 78931\n",
      "['▁현재', '▁쿠', '스', '꼬', '는', '▁페루', '▁북서', '쪽', '▁약', '▁64', 'km', '에', '▁있는', '▁밀림', '▁속', '▁마', '추', '픽', '추', '▁유적', '지와', '▁더불어', '▁페루', '의', '▁주요', '▁관광', '지다', '▁.'] ['▁today', '▁,', '▁cuzco', '▁is', '▁peru', '▁s', '▁main', '▁tourism', '▁hub', '▁and', '▁a', '▁launching', '▁point', '▁for', '▁visitors', '▁to', '▁the', '▁jungle', '▁shroud', 'ed', '▁ruins', '▁of', '▁mach', 'u', '▁pic', 'ch', 'u', '▁,', '▁40', '▁miles', '▁northwest', '▁.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "en_tensor.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(78931, 50)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 이렇게 케라스 토크나이저를 쓸 수도 있지만, 한번 sentencepiece를 조금 더 사용해보겠습니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def senten_tokenize(kor,eng, ko_model,en_model, max_len):\n",
    "    kos = []\n",
    "    ens = []\n",
    "    for i,j in zip(kor,eng):\n",
    "        i = preprocess_sentence(i)\n",
    "        j = preprocess_sentence(j)\n",
    "        ko = ko_model.EncodeAsIds(i)\n",
    "        en = en_model.EncodeAsIds(j)\n",
    "        if len(ko)>48 or len(en)>48: continue\n",
    "        kos.append(ko)\n",
    "        ens.append(en)\n",
    "    ko_tensor = tf.keras.preprocessing.sequence.pad_sequences(kos, padding='post',maxlen=max_len)\n",
    "    en_tensor = tf.keras.preprocessing.sequence.pad_sequences(ens, padding='post',maxlen=max_len)\n",
    "\n",
    "    return ko_tensor, en_tensor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 학습을 해도해도 결과가 안나와서 보니, 여기 전처리를 잘못했더라구요.\n",
    "## 모델도 중요하지만, 누가 대신해주는 거 없으니, 정신 똑바로 차려야하겠습니다!\n",
    "\n",
    "### decoder쪽 토그나이저는 set_encode_extra_options(\"bos:eos\")을 해주어 시작, 끝 토큰을 추가할 수 있습니다.\n",
    "### "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "en.set_encode_extra_options(\"bos:eos\")\n",
    "ko_tensor,en_tensor = senten_tokenize(kor,eng, ko,en, 50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BUFFER_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(ko_tensor, en_tensor, test_size=0.2) # 하... 여기 영어랑 한국어 순서 바뀌어 있었다..\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train,dec_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: ((128, 50), (128, 50)), types: (tf.int32, tf.int32)>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 50), (128, 50)), types: (tf.int32, tf.int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "train = dataset.__iter__()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "a = next(train)[0]\n",
    "print(a)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[  132 19422 18810 ...     0     0     0]\n",
      " [ 4955 18848 14651 ...     0     0     0]\n",
      " [ 2971 18765  4133 ...     0     0     0]\n",
      " ...\n",
      " [ 8557 18811  4674 ...     0     0     0]\n",
      " [ 3468  1884  3470 ...     0     0     0]\n",
      " [  308   322  9008 ...     0     0     0]], shape=(128, 50), dtype=int32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "transformer = Transformer(4,512,4,2048,20289,20001,50,shared=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 와... 위대하다.\n",
    "### 여기서 오류가 얼마나 떴는지 모르겠다.\n",
    "### super은 init이라는 메소드가 없다는 부분부터, pos가 정의되지 않았다는 내용 등등. 설계를 똑바로 하니, 틀린부분이 어디인지 알아서 찾아준다는 점이 아주 뿌듯하다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# ex = '''오바마는 대통령이다.\n",
    "# 시민들은 도시 속에 산다.\n",
    "# 커피는 필요 없다.\n",
    "# 일곱 명의 사망자가 발생했다.'''\n",
    "# ex = ex.split('\\n')\n",
    "# corpus = []\n",
    "# for i in ex:\n",
    "#     corpus.append(ko.SampleEncodeAsPieces(i,1, 0.0))\n",
    "# for i in corpus:\n",
    "#     print(i)\n",
    "# tensor = ko_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "# tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=50)\n",
    "# print(tensor)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()\n",
    "\n",
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 문장이 짧아서 오히려 잘 안될지도 모른다는 걱정이 들지만, 논리적이지 않은 걱정인 것 같습니다..ㅎㅎ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "transformer = Transformer(6,512,4,2048,20001,20001,50,shared=False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(ko_tensor, en_tensor, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train,dec_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset\n",
    "\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def eval_step(src,tgt, model):\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src,tgt)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = model(src,tgt,enc_mask, dec_enc_mask, dec_mask)\n",
    "    val_loss = loss_function(gold, predictions[:, :-1])\n",
    "    \n",
    "    \n",
    "    return val_loss, enc_attns, dec_attns, dec_enc_attns\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<BatchDataset shapes: ((128, 50), (128, 50)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    val_loss = 0\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))[:-2]\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "    train = dataset.__iter__()\n",
    "    val = val_dataset.__iter__()\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(*next(train),\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "\n",
    "    idx_list = list(range(0, enc_val.shape[0], BATCH_SIZE))[:-2]\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            eval_step(*next(val),\n",
    "                       transformer)\n",
    "        \n",
    "        val_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Val_Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Val_loss %.4f' % (val_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for ex in examples:\n",
    "        translate(ex, transformer, ko, en)\n",
    "    time.sleep(2)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  1: 100%|██████████| 458/458 [12:40<00:00,  1.66s/it, Loss 6.8884]\n",
      "Val_Epoch  1: 100%|██████████| 113/113 [01:03<00:00,  1.79it/s, Val_loss 6.5562]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the the the the the , the , the , the .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the is the the the , the\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the is the the the , the\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the the the the the , the\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  2: 100%|██████████| 458/458 [12:23<00:00,  1.62s/it, Loss 6.2497]\n",
      "Val_Epoch  2: 100%|██████████| 113/113 [01:00<00:00,  1.86it/s, Val_loss 6.0535]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the the situation , the new new lot of the united states .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the new korean korean of the country .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the dow is not the new new lot of the new new new country .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the new korean korean of the country .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  3: 100%|██████████| 458/458 [12:28<00:00,  1.63s/it, Loss 5.7760]\n",
      "Val_Epoch  3: 100%|██████████| 113/113 [01:00<00:00,  1.87it/s, Val_loss 5.7525]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama , the nomination .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the man was a second .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i re a kind of the same .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the suicide wounded wounded at the wounded .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  4: 100%|██████████| 458/458 [12:21<00:00,  1.62s/it, Loss 5.3606]\n",
      "Val_Epoch  4: 100%|██████████| 113/113 [01:00<00:00,  1.86it/s, Val_loss 5.5600]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: i is a lot of the democratic democratic democratic democratic party .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the man is a good .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i can do going to be .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the two people were injured .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  5: 100%|██████████| 458/458 [12:26<00:00,  1.63s/it, Loss 5.0596]\n",
      "Val_Epoch  5: 100%|██████████| 113/113 [01:00<00:00,  1.86it/s, Val_loss 5.4770]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama , obama cnn  ⁇ \n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the truth is not be .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it is a lot of the situation .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the wounded was killed .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  6: 100%|██████████| 458/458 [12:21<00:00,  1.62s/it, Loss 4.7976]\n",
      "Val_Epoch  6: 100%|██████████| 113/113 [01:00<00:00,  1.87it/s, Val_loss 5.3235]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president george obama , obama , and president bush .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the cause of the planet .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i can be safe .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: two people were killed .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  7: 100%|██████████| 458/458 [12:22<00:00,  1.62s/it, Loss 4.5435]\n",
      "Val_Epoch  7: 100%|██████████| 113/113 [01:00<00:00,  1.86it/s, Val_loss 5.2956]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama president george obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the indian panda is now .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: i can t be able to be a very dangerous .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: eight people were killed .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  8: 100%|██████████| 458/458 [12:34<00:00,  1.65s/it, Loss 4.3179]\n",
      "Val_Epoch  8: 100%|██████████| 113/113 [01:00<00:00,  1.88it/s, Val_loss 5.2840]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a president  ⁇ \n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the poison rooted is rooted .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: if you can be rid of course .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: eight people were wounded .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  9: 100%|██████████| 458/458 [12:19<00:00,  1.61s/it, Loss 4.0272]\n",
      "Val_Epoch  9: 100%|██████████| 113/113 [01:00<00:00,  1.86it/s, Val_loss 5.2648]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president obama is president bush .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the elephants are in the midst of the city of the city of hindu city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: if you can you you you you you you you you .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: eight people were killed .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 10: 100%|██████████| 458/458 [12:24<00:00,  1.63s/it, Loss 3.7156]\n",
      "Val_Epoch 10: 100%|██████████| 113/113 [01:00<00:00,  1.87it/s, Val_loss 5.3066]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the president is a president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the indian is rooted .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: if you can .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least seven people were killed .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 11: 100%|██████████| 458/458 [12:21<00:00,  1.62s/it, Loss 3.4239]\n",
      "Val_Epoch 11: 100%|██████████| 113/113 [01:00<00:00,  1.87it/s, Val_loss 5.3883]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama , who is president bush , president , obama , president , obama , is president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the threatens rooted in the middle of the middle .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: if you can store\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: eight people were killed .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 12: 100%|██████████| 458/458 [12:19<00:00,  1.62s/it, Loss 3.1460]\n",
      "Val_Epoch 12: 100%|██████████| 113/113 [01:00<00:00,  1.87it/s, Val_loss 5.5044]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is one of obama s inauguration .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: hundreds of people are in the park .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: if you can be able to ban .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 13: 100%|██████████| 458/458 [12:26<00:00,  1.63s/it, Loss 2.8855]\n",
      "Val_Epoch 13: 100%|██████████| 113/113 [01:00<00:00,  1.87it/s, Val_loss 5.6656]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: he s a dream of care , he told illinois , february care in indiana care .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: 82 year old kerts .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: improve , ban is more more .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 14: 100%|██████████| 458/458 [12:30<00:00,  1.64s/it, Loss 2.6426]\n",
      "Val_Epoch 14: 100%|██████████| 113/113 [01:00<00:00,  1.86it/s, Val_loss 5.7954]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: he is welcoming obama s lead in february .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: some of the fires are collapsed .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: improve coffee is not matched .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 15: 100%|██████████| 458/458 [12:17<00:00,  1.61s/it, Loss 2.4181]\n",
      "Val_Epoch 15: 100%|██████████| 113/113 [01:00<00:00,  1.88it/s, Val_loss 5.9435]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s annual october 29th .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: in the past week .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: improve your product is .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were injured .\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "checkpoint_dir = \"/aiffel/Data/Model/transformer/weight\"\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(transformer=transformer)\n",
    "\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/aiffel/Data/Model/transformer/weight/ckpt-1'"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers,d_model,\n",
    "                 n_heads,dff,src_vocab_size,\n",
    "                 tgt_vocab_size, pos_len, loss_function,\n",
    "                 dropout=0.2, shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_embedding = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_embedding = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.positional = positional_encoding(pos_len,d_model)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, dff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, dff, dropout)\n",
    "        \n",
    "        self.out_linear = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(dropout/2)\n",
    "\n",
    "        self.shared = shared\n",
    "        self.history = {'loss':[],'val_loss':[], 'attention':[]}\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        if shared: self.out_linear.set_weights(tf.transpose(self.dec_embedding.weights)) # 이런 생각을 한다는 것이 매우 놀랍다.\n",
    "\n",
    "    \n",
    "    def embedding(self, emb, x,training=False):\n",
    "        # share?\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.positional[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out, training=training)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask, training=False):\n",
    "        # 1 embedding\n",
    "        enc = self.embedding(self.enc_embedding, enc_in, training)\n",
    "        dec = self.embedding(self.dec_embedding, dec_in, training)\n",
    "\n",
    "        # 2 encoder, decoder\n",
    "        enc_out, enc_attns = self.encoder(enc, enc_mask, training)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec, enc_out, causality_mask, dec_mask, training)\n",
    "\n",
    "        logits = self.out_linear(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "    \n",
    "    def generate_padding_mask(self, seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def generate_causality_mask(self, src_len, tgt_len):\n",
    "        mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "        return tf.cast(mask, tf.float32)\n",
    "\n",
    "    def generate_masks(self, src, tgt):\n",
    "        enc_mask = self.generate_padding_mask(src)\n",
    "        dec_mask = self.generate_padding_mask(tgt)\n",
    "\n",
    "        dec_enc_causality_mask = self.generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "        dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "        dec_causality_mask = self.generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "        dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "        return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "    @tf.function()\n",
    "    def train_step(self, src, tgt, optimizer):\n",
    "        gold = tgt[:, 1:]\n",
    "            \n",
    "        enc_mask, dec_enc_mask, dec_mask = self.generate_masks(src, tgt)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, enc_attns, dec_attns, dec_enc_attns = self(src, tgt, enc_mask, dec_enc_mask, dec_mask, training=True)\n",
    "            loss = self.loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)    \n",
    "        optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @tf.function()\n",
    "    def eval_step(self, src,tgt):\n",
    "        gold = tgt[:, 1:]\n",
    "\n",
    "        enc_mask, dec_enc_mask, dec_mask = self.generate_masks(src,tgt)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = self(src,tgt,enc_mask, dec_enc_mask, dec_mask)\n",
    "        val_loss = self.loss_function(gold, predictions[:, :-1])\n",
    "        \n",
    "        \n",
    "        return val_loss, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "\n",
    "    def fit(self, epochs=20, x_train=None, y_train=None,\n",
    "            x_val=None, y_val=None, BATCH_SIZE=128, \n",
    "            offset_epoch=0, translate=None, examples=None):\n",
    "        EPOCHS = epochs\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            total_loss = 0\n",
    "            val_loss = 0\n",
    "            idx_list = list(range(0, x_train.shape[0], BATCH_SIZE))\n",
    "            random.shuffle(idx_list)\n",
    "            t = tqdm(idx_list)\n",
    "\n",
    "            for (batch, idx) in enumerate(t):\n",
    "                batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "                self.train_step(x_train[idx:idx+BATCH_SIZE],\n",
    "                                y_train[idx:idx+BATCH_SIZE],\n",
    "                                optimizer)\n",
    "\n",
    "                total_loss += batch_loss\n",
    "                \n",
    "                t.set_description_str('Epoch %2d' % (offset_epoch+epoch + 1))\n",
    "                t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "            self.history['loss'].append(total_loss.numpy() / (batch + 1))\n",
    "\n",
    "            if x_val is not None:\n",
    "                idx_list = list(range(0, x_val.shape[0], BATCH_SIZE))[:-2]\n",
    "                random.shuffle(idx_list)\n",
    "                t = tqdm(idx_list)\n",
    "\n",
    "                for (batch, idx) in enumerate(t):\n",
    "                    batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "                        self.eval_step(x_val[idx:idx+BATCH_SIZE],\n",
    "                                    y_val[idx:idx+BATCH_SIZE])\n",
    "                    \n",
    "                    val_loss += batch_loss\n",
    "                    \n",
    "                    t.set_description_str('Val_epoch %2d' % (offset_epoch+epoch + 1))\n",
    "                    t.set_postfix_str('Val_loss %.4f' % (val_loss.numpy() / (batch + 1)))\n",
    "                self.history['val_loss'].append(val_loss.numpy() / (batch + 1))\n",
    "\n",
    "            if examples and translate:\n",
    "                for ex in examples:\n",
    "                    translate(ex, transformer, ko, en)\n",
    "                time.sleep(2)\n",
    "\n",
    "        self.history['attention'].extend([enc_attns, dec_attns, dec_enc_attns])\n",
    "        return self.history\n",
    "\n",
    "\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "transformer = Transformer(n_layers=4, d_model=512, n_heads=4, dff=2048, \n",
    "                          src_vocab_size=20000, tgt_vocab_size=20000, \n",
    "                          pos_len=50, loss_function=loss_function, dropout=0.4, shared=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(ko_tensor, en_tensor, test_size=0.1)\n",
    "\n",
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\",\n",
    "            \"심심해서 당신이 보고싶다.\"\n",
    "]\n",
    "\n",
    "Thanks = transformer.fit(10, enc_train, dec_train, enc_val, dec_val, BATCH_SIZE, translate=translate, examples=examples)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  1: 100%|██████████| 259/259 [09:10<00:00,  2.13s/it, Loss 8.1188]\n",
      "Val_epoch  1: 100%|██████████| 27/27 [00:20<00:00,  1.35it/s, Val_loss 7.0056]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the the the . . .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the the the the .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the the the . . .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the the the . . .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the the the the the .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  2: 100%|██████████| 259/259 [09:23<00:00,  2.17s/it, Loss 7.0627]\n",
      "Val_epoch  2: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 6.8395]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the the the . . .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the the the . . .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the the the . . .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the the the . . .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the the the the . .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  3: 100%|██████████| 259/259 [09:15<00:00,  2.15s/it, Loss 6.7879]\n",
      "Val_epoch  3: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 6.7998]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  4: 100%|██████████| 259/259 [08:54<00:00,  2.06s/it, Loss 6.5628]\n",
      "Val_epoch  4: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 6.6455]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the dow of the be\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the the united , the the the the the the the the world .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the the world of the be\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the the united states of the the the the be the be the world .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the the united , the the the the the the world .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  5: 100%|██████████| 259/259 [08:54<00:00,  2.07s/it, Loss 6.3414]\n",
      "Val_epoch  5: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 6.6536]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the dow is the end of the be the end .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the dow is the end .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the dow is the end of the be the end .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the dow is the end of the be the end .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the dow is the first world .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  6: 100%|██████████| 259/259 [09:09<00:00,  2.12s/it, Loss 6.1136]\n",
      "Val_epoch  6: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 6.3553]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the dow is the first korean the first korean the first korean the united states .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: but the dow is the be the first korean be the first korean be korean the united states .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: but he was t been t been t been t t very be the be the be the be the end .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the dow is the first korean the first korean the first korean the united states to be the end it was the end .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the dow is the first korean the united states to be the end .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  7: 100%|██████████| 259/259 [09:37<00:00,  2.23s/it, Loss 5.8963]\n",
      "Val_epoch  7: 100%|██████████| 27/27 [00:28<00:00,  1.05s/it, Val_loss 6.1752]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the dow is a lot of the world s end to be a nuclear nomination .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the dow is the world .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: he was not to be be a lot .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the dow was expected to meet the united states s nuclear nuclear nuclear nuclear nuclear nuclear nuclear nuclear korean korean korean korean korean korean korean korean korean korean korean korean korean korean korean korean korean nuclear nuclear nuclear nuclear nuclear nuclear nuclear korean korean korean korean weapons .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the dow is the world of the world .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  8: 100%|██████████| 259/259 [09:23<00:00,  2.18s/it, Loss 5.6844]\n",
      "Val_epoch  8: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 6.0466]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the korea herald .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the dow is a lot of the world s be a lot of the world s end it is the first be half it s be a lead it s be a be half the economy .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the ap .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the dow were killed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap correspondent .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch  9: 100%|██████████| 259/259 [09:08<00:00,  2.12s/it, Loss 5.4958]\n",
      "Val_epoch  9: 100%|██████████| 27/27 [00:18<00:00,  1.44it/s, Val_loss 5.9298]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the white house says the senate is a candidate .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: but the ap , the ap , is a new first half .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: but you is going to be a lot .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the government has been reported .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap s ap .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 10: 100%|██████████| 259/259 [09:15<00:00,  2.15s/it, Loss 5.3298]\n",
      "Val_epoch 10: 100%|██████████| 27/27 [00:19<00:00,  1.40it/s, Val_loss 5.7177]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a very time of the white house .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the ap s mark is a lot of the world s largest press .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: we don t know we re sorry to do you re .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the government herald reports it was no estimated least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least least 750 people .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap s mark smith ⁇ \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "checkpoint_dir = \"/aiffel/Data/Model/transformer/weight\"\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"uplayer4\")\n",
    "checkpoint = tf.train.Checkpoint(transformer=transformer)\n",
    "\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/aiffel/Data/Model/transformer/weight/uplayer4-1'"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "Thanks = transformer.fit(10, enc_train, dec_train, enc_val, dec_val, BATCH_SIZE, translate=translate, examples=examples, offset_epoch=10)\n",
    "\n",
    "checkpoint_dir = \"/aiffel/Data/Model/transformer/weight\"\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"uplayer4-2\")\n",
    "checkpoint = tf.train.Checkpoint(transformer=transformer)\n",
    "\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 11: 100%|██████████| 259/259 [09:52<00:00,  2.29s/it, Loss 5.1868]\n",
      "Val_epoch 11: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 5.6006]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s obama times .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the cause of the planet .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: if you can be able to be able to be able to be .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the cause of the blast were killed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: i think you re a lot of your own your own .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 12: 100%|██████████| 259/259 [08:55<00:00,  2.07s/it, Loss 5.0613]\n",
      "Val_epoch 12: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 5.5729]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the president elect elect elect elect elect elect sauger bak moo candidate .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the ap s jason correspondent station has been killed .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a very time .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the blasts occurred the dead of the town of the town of the outskirts of the town of the town of the town of the town of the town .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap s jerry bodlander old correspondent jerry bodlander .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 13: 100%|██████████| 259/259 [09:53<00:00,  2.29s/it, Loss 4.9460]\n",
      "Val_epoch 13: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 5.3919]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the senator s mark smith senator reports he was too a new first time .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the storm was a bit of the storm .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a lot of the problem .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the storm was destroyed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap s mark smith\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 14: 100%|██████████| 259/259 [08:57<00:00,  2.07s/it, Loss 4.8397]\n",
      "Val_epoch 14: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 5.3217]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama has been a new aide for barack obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the ap s jason correspondent has been found .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a lot of people .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the official reports the associated press this week .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap s bryant thomas thomas says it s not a good friend .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 15: 100%|██████████| 259/259 [09:08<00:00,  2.12s/it, Loss 4.7271]\n",
      "Val_epoch 15: 100%|██████████| 27/27 [00:18<00:00,  1.42it/s, Val_loss 5.3720]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s campaign has a tough debate .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they are still trying to get a fire .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a lot of people .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the blast occurred at least 11 , 000 people and the capital of the capital of the capital of the capital of peshawar .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap s sauger magoni\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 16: 100%|██████████| 259/259 [09:20<00:00,  2.16s/it, Loss 4.6231]\n",
      "Val_epoch 16: 100%|██████████| 27/27 [00:19<00:00,  1.42it/s, Val_loss 5.1729]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is also a very popular aide .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the woman was carrying a large man .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s a lot of brain .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll in the philippines suburb of the capital province suburb , killing at least 1 , 800 people were killed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap s jason reed\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 17: 100%|██████████| 259/259 [09:43<00:00,  2.25s/it, Loss 4.4934]\n",
      "Val_epoch 17: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 5.1515]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama says he was disappointed .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the woman s death toll .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s a little bit .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the protesters were killed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap s jason correspondent sagar meghani .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 18: 100%|██████████| 259/259 [09:07<00:00,  2.11s/it, Loss 4.3651]\n",
      "Val_epoch 18: 100%|██████████| 27/27 [00:20<00:00,  1.32it/s, Val_loss 5.0165]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama has been undergoing barack obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the two people were injured .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a good fix .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was destroyed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: it s a good time .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 19: 100%|██████████| 259/259 [09:10<00:00,  2.13s/it, Loss 4.2433]\n",
      "Val_epoch 19: 100%|██████████| 27/27 [00:19<00:00,  1.41it/s, Val_loss 5.0867]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s campaign has a chance to obama s campaign .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the street is the only one of the most unusual buildings .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a lot of heart .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the deadliest attack in the town of the country s deadliest history .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap s rita foley is not a tricky class .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 20: 100%|██████████| 259/259 [09:04<00:00,  2.10s/it, Loss 4.1330]\n",
      "Val_epoch 20: 100%|██████████| 27/27 [00:18<00:00,  1.44it/s, Val_loss 5.0159]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s campaign is a new president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the street is a huge part of the street .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a good fix .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was reported .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the ap s jason gramm reports about the age of the story\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/aiffel/Data/Model/transformer/weight/uplayer4-2-1'"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "Thanks = transformer.fit(10, enc_train, dec_train, enc_val, dec_val, BATCH_SIZE, translate=translate, examples=examples, offset_epoch=20)\n",
    "\n",
    "checkpoint_dir = \"/aiffel/Data/Model/transformer/weight\"\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"uplayer4-3\")\n",
    "checkpoint = tf.train.Checkpoint(transformer=transformer)\n",
    "\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 21: 100%|██████████| 259/259 [09:13<00:00,  2.14s/it, Loss 4.0282]\n",
      "Val_epoch 21: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 5.0457]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama has been a good supporter of the obama administration .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protesters were crowded .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a lot of brain .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: more than least 200 people were killed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the idea of the experiment is a problem .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 22: 100%|██████████| 259/259 [09:07<00:00,  2.11s/it, Loss 3.9270]\n",
      "Val_epoch 22: 100%|██████████| 27/27 [00:20<00:00,  1.30it/s, Val_loss 4.9712]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama ⁇ s elected national nominee barack obama says he wants to reverse whether he wants to reverse the president elect s job .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the roof of the kitchen was a small city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a lot of brain .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least 61 people were killed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: the bad consideration is that the bad .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 23: 100%|██████████| 259/259 [09:27<00:00,  2.19s/it, Loss 3.8384]\n",
      "Val_epoch 23: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 4.9806]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama ⁇ s rogue president elect barack obama\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protesters were furious .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a lot of fun .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least 15 people have been killed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: and you know you ll be able to get a personal job .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 24: 100%|██████████| 259/259 [09:07<00:00,  2.11s/it, Loss 3.7502]\n",
      "Val_epoch 24: 100%|██████████| 27/27 [00:18<00:00,  1.44it/s, Val_loss 4.9941]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s campaign has a new president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the rally was a huge night .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a lot of people .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was blamed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: you know you re gonna be a sociator .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 25: 100%|██████████| 259/259 [09:29<00:00,  2.20s/it, Loss 3.6695]\n",
      "Val_epoch 25: 100%|██████████| 27/27 [00:19<00:00,  1.42it/s, Val_loss 4.9507]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama has been elected .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protesters are packed to be the first .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a lot of brain .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the deadliest effect toll from the weekend .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: it s a simple sense of the story .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 26: 100%|██████████| 259/259 [09:09<00:00,  2.12s/it, Loss 3.5902]\n",
      "Val_epoch 26: 100%|██████████| 27/27 [00:18<00:00,  1.45it/s, Val_loss 4.9778]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama ⁇ s elected slam\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protesters were crowded .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a good precedent .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the deadliest effect toll from the u . s .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: otherwise , it s a good sense of fun .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 27: 100%|██████████| 259/259 [09:07<00:00,  2.11s/it, Loss 3.5153]\n",
      "Val_epoch 27: 100%|██████████| 27/27 [00:19<00:00,  1.39it/s, Val_loss 4.9864]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama replied he wants to reverse whether he was going to reverse obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protests are concentrated on the street .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not true .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was reported to be a week .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: you don t heard about your job .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 28: 100%|██████████| 259/259 [09:32<00:00,  2.21s/it, Loss 3.4468]\n",
      "Val_epoch 28: 100%|██████████| 27/27 [00:18<00:00,  1.44it/s, Val_loss 4.9920]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama has been elected president elect barack obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the rally is a day after the city s snow .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a good problem .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least 23 people were killed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: otherwise , it s a good question .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 29: 100%|██████████| 259/259 [09:15<00:00,  2.14s/it, Loss 3.3809]\n",
      "Val_epoch 29: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 4.9930]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the first president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protests are the worst internal city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a very good problem .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was concealed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: bad news for the concept of reaction to the simple .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 30: 100%|██████████| 259/259 [08:54<00:00,  2.06s/it, Loss 3.3182]\n",
      "Val_epoch 30: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 4.9860]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s aides say they re lucky to reverse what he called on .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: protesters were demolving .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a good thing .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was reported .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: you can see you to see you .\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/aiffel/Data/Model/transformer/weight/uplayer4-3-1'"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "Thanks = transformer.fit(3, enc_train, dec_train, enc_val, dec_val, BATCH_SIZE, translate=translate, examples=examples, offset_epoch=30)\n",
    "\n",
    "checkpoint_dir = \"/aiffel/Data/Model/transformer/weight\"\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"uplayer4-4\")\n",
    "checkpoint = tf.train.Checkpoint(transformer=transformer)\n",
    "\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 31: 100%|██████████| 259/259 [09:01<00:00,  2.09s/it, Loss 3.2591]\n",
      "Val_epoch 31: 100%|██████████| 27/27 [00:18<00:00,  1.46it/s, Val_loss 5.0202]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama ⁇ s active\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protesters are riding to vote .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a good thing .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least 20 people were killed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: you can t see that .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 32: 100%|██████████| 259/259 [09:04<00:00,  2.10s/it, Loss 3.2054]\n",
      "Val_epoch 32: 100%|██████████| 27/27 [00:19<00:00,  1.41it/s, Val_loss 5.0162]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama ⁇ s a president elect .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: protesters are staying in the midst of the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a lot of brain .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least 15 people have died .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: you can tell you to do it .\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 33: 100%|██████████| 259/259 [08:58<00:00,  2.08s/it, Loss 3.1517]\n",
      "Val_epoch 33: 100%|██████████| 27/27 [00:19<00:00,  1.41it/s, Val_loss 5.0151]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s camp touted a second consecutive term\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the protests are concentrated in the crucwing of the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s not a good thing .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: at least seven people have been killed .\n",
      "Input: 심심해서 당신이 보고싶다.\n",
      "Predicted translation: it s a good story .\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/aiffel/Data/Model/transformer/weight/uplayer4-4-1'"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. 회고\n",
    "## 1) 이걸로 며칠을 보내버렸는지 모르겠다. 처음 학습을 돌리고, 왜 종료토큰을 학습하지 못하는지 의문이 들었는데, 종료토큰을 학습한 적이 없었기 때문이였다. 참.... 세상에 이유없는 일은 없는 것인지도 모른다는 생각이 들었다.\n",
    "## 2) 내 아이가 언어를 익힌 것 같은 기쁨이 들었다."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "interpreter": {
   "hash": "026aceb1435085fcef523649cdfc9385a4a55dbc5c65435142607853821fa50a"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}